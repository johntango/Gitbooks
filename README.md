---
description: Taken from Dale on AI
---

# Transformers

{% embed url="https://daleonai.com/transformers-explained" %}
from Daleonai.com
{% endembed %}

![](.gitbook/assets/TransformerArchitecture.png)

* Positional Encodings
  * \[("The", 1), ("aliens", 2), ("say", 3), ("hello", 4)]
  * Put structure into the data
* Attention
  * _The agreement on the European Economic Area was signed in August 1992._
  * _L’accord sur la zone économique européenne a été signé en août 1992._



![](.gitbook/assets/Attention001.png)

* Self-Attention
  * “Server, can I have the check?”
  * “Looks like I just crashed the server.”

### History - A Hammer for all nails

BERT, short for “Bidirectional Encoder Representations from Transformers”, was introduced by researchers at Google around 2018

{% embed url="https://github.com/google-research/bert" %}
BERT - a free trained model
{% endembed %}

